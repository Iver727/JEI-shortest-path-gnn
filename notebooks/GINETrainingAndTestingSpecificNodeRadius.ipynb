{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "yh3BbGigEKjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install captum # causes dependency issue with numpy as numpy requires a version <2; in colab simply hit restart runtime to use the older version without error\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "BYwVBMBKC_Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viaoM-SzJaTG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from itertools import pairwise, product\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as pyg\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    return seed\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "uGMO6qcPJg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores data with all values pertaining to their individual training\n",
        "experiments = []\n",
        "\n",
        "def add_experiment(param):\n",
        "    experiments.append(param)\n",
        "\n",
        "def get_dataframe():\n",
        "    df = pd.DataFrame(experiments)\n",
        "    return df\n",
        "\n",
        "def clear_experiments():\n",
        "    experiments.clear()\n",
        "\n",
        "def save_dataframe(path):\n",
        "    df = get_dataframe()\n",
        "    df.to_csv(path)"
      ],
      "metadata": {
        "id": "S280qlJFJivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell must change depending on runtime environment; it is currently configured for Colab.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "folder_path = \"/content/drive/MyDrive/FinDSExperiments/GINE_Size_Applicability_by_NumHops/\""
      ],
      "metadata": {
        "id": "4Q53TE_zJumj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "eiqKCnDrEjx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShortestPathGNN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        num_layers: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in_channels = 1\n",
        "        out_channels = 1\n",
        "        self.encoder = torch.nn.Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            layer = pyg.nn.GINEConv(\n",
        "                nn=torch.nn.Sequential(\n",
        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "                ),edge_dim=1)\n",
        "            self.layers.append(layer)\n",
        "        self.decoder = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor):\n",
        "        x = self.encoder(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, edge_index, edge_attr)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KNFON27fKH6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Data"
      ],
      "metadata": {
        "id": "V7-RVeXw7gxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import KeysView\n",
        "K = 4\n",
        "large_number = 10000\n",
        "\n",
        "def convert_networkx_to_pyg_shortest_path(graph: nx.Graph,large_number:int): # Modification here to also store num_hops for each node.\n",
        "    nx.set_edge_attributes(graph, values={e : 1.0 + 0.1*np.random.randn() for e in graph.edges()}, name='edge_attr')\n",
        "    data = pyg.utils.convert.from_networkx(graph)\n",
        "    data.x = torch.Tensor([0] + [large_number for _ in range(data.num_nodes-1)]).unsqueeze(1)\n",
        "    length_dict = nx.shortest_path_length(graph, source=0, weight=\"edge_attr\")\n",
        "    data.y = torch.Tensor([length_dict.get(i, large_number) for i in range(data.num_nodes)])\n",
        "    data.edge_attr = data.edge_attr.unsqueeze(1)\n",
        "    num_hops_dict = nx.shortest_path_length(graph, source=0)\n",
        "    data.num_hops = torch.tensor([num_hops_dict.get(i, large_number) for i in range(data.num_nodes)])\n",
        "    return data\n",
        "\n",
        "def get_connected_ER_graph(num_nodes: int, p: float):\n",
        "    while True: # loop until we generate a connected graph\n",
        "        graph = nx.erdos_renyi_graph(num_nodes, p)\n",
        "        if nx.is_connected(graph):\n",
        "            return graph\n",
        "\n",
        "def create_pyg_dataset(num_nodes: int, num_samples: int, large_number:int):\n",
        "    set_seed(0)\n",
        "    return [\n",
        "        convert_networkx_to_pyg_shortest_path(get_connected_ER_graph(num_nodes, p=0.1),large_number)\n",
        "        for _ in range(num_samples)\n",
        "    ]"
      ],
      "metadata": {
        "id": "vLSiSyZUKLlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom experimental setup code:"
      ],
      "metadata": {
        "id": "DdvEO2PYLAPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_node_sizes = []\n",
        "testing_node_sizes = []\n",
        "\n",
        "value = 20.0;\n",
        "while(value<1000):\n",
        "    training_node_sizes.append(int(value))\n",
        "    value*=1.2\n",
        "\n",
        "value = 5.0;\n",
        "while(value<2720):\n",
        "    testing_node_sizes.append(int(value))\n",
        "    value *= 1.3"
      ],
      "metadata": {
        "id": "VubXf4p8KXFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(0)\n",
        "\n",
        "# Generate testing data\n",
        "testing_data_by_sizes = {}\n",
        "for testing_node_size in testing_node_sizes:\n",
        "    num_test_samples = 100\n",
        "    test_dataset = create_pyg_dataset(num_nodes=testing_node_size, num_samples=num_test_samples,large_number=large_number)\n",
        "    testing_data_by_sizes[testing_node_size] = test_dataset"
      ],
      "metadata": {
        "id": "CfRcKR7jnAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(0)\n",
        "\n",
        "# Generate training data\n",
        "training_data_by_sizes = {}\n",
        "for training_node_size in training_node_sizes:\n",
        "    num_train_samples = 100\n",
        "    train_dataset = create_pyg_dataset(num_nodes=training_node_size, num_samples=num_train_samples,large_number=large_number)\n",
        "    training_data_by_sizes[training_node_size] = train_dataset"
      ],
      "metadata": {
        "id": "Awfi8GagTbW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hops_max_in_train_graphs = 0\n",
        "\n",
        "for key, graphs in training_data_by_sizes.items():\n",
        "    for graph in graphs:\n",
        "        for value in graph.num_hops:\n",
        "            if value.item() > hops_max_in_train_graphs:\n",
        "                hops_max_in_train_graphs = value.item()\n",
        "\n",
        "print(hops_max_in_train_graphs)"
      ],
      "metadata": {
        "id": "RNxQjGowUizo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training/Testing"
      ],
      "metadata": {
        "id": "f3LAcFQh7vHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom experimental setup code:"
      ],
      "metadata": {
        "id": "-dTLQe_6K58w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "networks = {}\n",
        "optimizers = {}"
      ],
      "metadata": {
        "id": "PDADDlekZjqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training basic setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "loss_function = torch.nn.MSELoss(reduction='none')\n",
        "\n",
        "def temp_test(network, test_loader):\n",
        "    network.eval()\n",
        "    test_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "            loss = loss_function(pred.flatten(), batch.y)\n",
        "            test_loss += loss.mean().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    return test_loss\n",
        "\n",
        "def train_one_epoch(number, network, optimizer, train_loader, alpha):\n",
        "        network.train()\n",
        "        epoch_loss = 0\n",
        "        num_adds = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            network.zero_grad()\n",
        "            batch = batch.to(device)\n",
        "            pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "\n",
        "            target_node_mask = (batch.num_hops == number)\n",
        "\n",
        "            if target_node_mask.sum() > 0:\n",
        "                masked_pred = pred.flatten()[target_node_mask]\n",
        "                masked_y = batch.y[target_node_mask]\n",
        "\n",
        "                individual_losses = loss_function(masked_pred, masked_y)\n",
        "                loss = individual_losses.sum()\n",
        "\n",
        "                # Add L1 regularization\n",
        "                l1_norm = sum(p.abs().sum() for p in network.parameters())\n",
        "                loss = loss + alpha * l1_norm\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                num_adds += target_node_mask.sum().item()\n",
        "\n",
        "        if num_adds == 0:\n",
        "            print(f\"No training data for this batch in network {number}\")\n",
        "            return None\n",
        "\n",
        "        epoch_loss /= num_adds\n",
        "        return epoch_loss\n",
        "\n",
        "def runTraining(hidden_channels_choice, seed, alpha):\n",
        "    set_seed(seed)\n",
        "    network = ShortestPathGNN(hidden_channels=hidden_channels_choice, num_layers=K)\n",
        "    network.to(device)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=0.0003)\n",
        "    num_epochs = 1000\n",
        "\n",
        "    test_loader_temp = pyg.loader.DataLoader(testing_data_by_sizes[14], batch_size=64, shuffle=False) # for testing during training\n",
        "\n",
        "    for key in range(hops_max_in_train_graphs+1):\n",
        "        set_seed(seed)\n",
        "        networks[key] = ShortestPathGNN(hidden_channels=hidden_channels_choice, num_layers=K)\n",
        "        networks[key].to(device)\n",
        "        optimizers[key] = torch.optim.Adam(networks[key].parameters(), lr=0.0003)\n",
        "\n",
        "    pbar = tqdm(range(num_epochs*len(networks.keys())*len(training_node_sizes)))\n",
        "    pbars = {}\n",
        "    for key in range(hops_max_in_train_graphs+1):\n",
        "        pbars[key] = tqdm(range(num_epochs))\n",
        "\n",
        "    for training_node_size in training_node_sizes: # makes sure all data is run through\n",
        "        train_loader = pyg.loader.DataLoader(training_data_by_sizes[training_node_size], batch_size=64, shuffle=True)\n",
        "        for key in networks.keys():\n",
        "            for epoch in pbars[key]:\n",
        "                networks[key].to(device)\n",
        "                train_loss = train_one_epoch(key, networks[key], optimizers[key], train_loader, alpha)\n",
        "                if train_loss is None:\n",
        "                    pbar.update(num_epochs - epoch)\n",
        "                    pbars[key].update(num_epochs)\n",
        "                    break\n",
        "                test_loss_temp = temp_test(networks[key], test_loader_temp)\n",
        "                train_loss_str = f\"{train_loss:.4f}\" if train_loss is not None else \"None\"\n",
        "                pbars[key].set_description(f\"Key: {key}, Epoch {epoch}, Train Loss: {train_loss_str}, Test Loss: {test_loss_temp:.4f}\")\n",
        "                pbar.set_description(f\"Key: {key}\")\n",
        "                pbar.update()\n",
        "        if training_node_size != training_node_sizes[-1]:\n",
        "            for key, ind_pbar in pbars.items():\n",
        "                pbars[key] = tqdm(range(num_epochs)) # Reset pbars for next training_node_size\n",
        "\n",
        "def test_model(network, loss_function, test_loader):\n",
        "    network.eval()\n",
        "    node_results = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "\n",
        "            individual_losses = loss_function(pred.flatten(), batch.y)\n",
        "            num_hops_batch = batch.num_hops\n",
        "\n",
        "            for i in range(pred.shape[0]):\n",
        "                node_results.append([num_hops_batch[i].item(), individual_losses[i].item()])\n",
        "    return node_results\n",
        "\n",
        "def run_testing_by_model(network, network_num):\n",
        "    test_losses_sum = {}\n",
        "    test_losses_total_adds = {}\n",
        "\n",
        "    pbar = tqdm(range(len(testing_data_by_sizes.keys())))\n",
        "    pbar.set_description(f\"Network Num Hops: {network_num}\")\n",
        "\n",
        "    for key,testing_data in testing_data_by_sizes.items():\n",
        "        test_loader = pyg.loader.DataLoader(testing_data, batch_size=64, shuffle=False)\n",
        "        test_losses_by_node = test_model(network, loss_function, test_loader)\n",
        "        for i in test_losses_by_node:\n",
        "            if not i[0] in test_losses_total_adds.keys():\n",
        "                test_losses_sum[i[0]] = i[1]\n",
        "                test_losses_total_adds[i[0]] = 1\n",
        "            else:\n",
        "                test_losses_sum[i[0]] += i[1]\n",
        "                test_losses_total_adds[i[0]] += 1\n",
        "        pbar.update()\n",
        "\n",
        "    test_losses_average = {i:test_losses_sum[i]/test_losses_total_adds[i] for i in test_losses_total_adds.keys()}\n",
        "\n",
        "    return test_losses_average\n"
      ],
      "metadata": {
        "id": "831Q_V2NKVrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels = 4\n",
        "alpha = 1.5\n",
        "\n",
        "runTraining(hidden_channels, 0, alpha) # Runs all training simultaneously to make better use of data loading"
      ],
      "metadata": {
        "id": "sqPYukCBE7hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses_by_training_num_hops = {}\n",
        "\n",
        "for key in networks.keys():\n",
        "    test_losses_by_training_num_hops[key] = run_testing_by_model(networks[key], key)\n",
        "    print(f\"Completed testing network: {key}\")"
      ],
      "metadata": {
        "id": "H7qJFowGajUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses_data = []\n",
        "row_labels = []\n",
        "column_labels = []\n",
        "for key, val in test_losses_by_training_num_hops.items():\n",
        "    if not column_labels:\n",
        "        column_labels = list(val.keys())\n",
        "    test_losses_data.append(list(val.values()))\n",
        "    row_labels.append(key)\n",
        "\n",
        "test_losses_array = np.array(test_losses_data)\n",
        "\n",
        "panda_array = pd.DataFrame(test_losses_array, index=row_labels, columns=column_labels)\n",
        "panda_array.to_csv(folder_path + \"test_losses_num_hops.csv\")"
      ],
      "metadata": {
        "id": "7qC9IQ-lFrCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}