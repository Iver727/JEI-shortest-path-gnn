{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "yh3BbGigEKjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install captum # causes dependency issue with numpy as numpy requires a version <2; in colab simply hit restart runtime to use the older version without error\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "BYwVBMBKC_Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viaoM-SzJaTG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from itertools import pairwise, product\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as pyg\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    return seed\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "uGMO6qcPJg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores data with all values pertaining to their individual training\n",
        "experiments = []\n",
        "\n",
        "def add_experiment(param):\n",
        "    experiments.append(param)\n",
        "\n",
        "def get_dataframe():\n",
        "    df = pd.DataFrame(experiments)\n",
        "    return df\n",
        "\n",
        "def clear_experiments():\n",
        "    experiments.clear()\n",
        "\n",
        "def save_dataframe(path):\n",
        "    df = get_dataframe()\n",
        "    df.to_csv(path)"
      ],
      "metadata": {
        "id": "S280qlJFJivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell must change depending on runtime environment; it is currently configured for Colab.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "folder_path = \"/content/drive/MyDrive/FinDSExperiments/GINE_Size_Applicability/\""
      ],
      "metadata": {
        "id": "4Q53TE_zJumj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "eiqKCnDrEjx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShortestPathGNN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        num_layers: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in_channels = 1\n",
        "        out_channels = 1\n",
        "        self.encoder = torch.nn.Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            layer = pyg.nn.GINEConv(\n",
        "                nn=torch.nn.Sequential(\n",
        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "                ),edge_dim=1)\n",
        "            self.layers.append(layer)\n",
        "        self.decoder = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor):\n",
        "        x = self.encoder(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, edge_index, edge_attr)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KNFON27fKH6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Data"
      ],
      "metadata": {
        "id": "V7-RVeXw7gxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import KeysView\n",
        "K = 4\n",
        "large_number = 10000\n",
        "\n",
        "def convert_networkx_to_pyg_shortest_path(graph: nx.Graph,large_number:int):\n",
        "    nx.set_edge_attributes(graph, values={e : 1.0 + 0.1*np.random.randn() for e in graph.edges()}, name='edge_attr')\n",
        "    data = pyg.utils.convert.from_networkx(graph)\n",
        "    data.x = torch.Tensor([0] + [large_number for _ in range(data.num_nodes-1)]).unsqueeze(1)\n",
        "    length_dict = nx.shortest_path_length(graph, source=0, weight=\"edge_attr\")\n",
        "    data.y = torch.Tensor([length_dict.get(i, large_number) for i in range(data.num_nodes)])\n",
        "    data.edge_attr = data.edge_attr.unsqueeze(1)\n",
        "    return data\n",
        "\n",
        "def get_connected_ER_graph(num_nodes: int, p: float):\n",
        "    while True: # loop until we generate a connected graph\n",
        "        graph = nx.erdos_renyi_graph(num_nodes, p)\n",
        "        if nx.is_connected(graph):\n",
        "            return graph\n",
        "\n",
        "def create_pyg_dataset(num_nodes: int, num_samples: int, large_number:int):\n",
        "    set_seed(0)\n",
        "    return [\n",
        "        convert_networkx_to_pyg_shortest_path(get_connected_ER_graph(num_nodes, p=0.1),large_number)\n",
        "        for _ in range(num_samples)\n",
        "    ]"
      ],
      "metadata": {
        "id": "vLSiSyZUKLlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom experimental setup code:"
      ],
      "metadata": {
        "id": "DdvEO2PYLAPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_node_sizes = []\n",
        "testing_node_sizes = []\n",
        "\n",
        "value = 20.0;\n",
        "while(value<1000):\n",
        "    training_node_sizes.append(int(value))\n",
        "    value*=1.2\n",
        "\n",
        "value = 5.0;\n",
        "while(value<2720):\n",
        "    testing_node_sizes.append(int(value))\n",
        "    value *= 1.3"
      ],
      "metadata": {
        "id": "VubXf4p8KXFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(0)\n",
        "\n",
        "# Generate testing data\n",
        "testing_data_by_sizes = {}\n",
        "for testing_node_size in testing_node_sizes:\n",
        "    num_test_samples = 100\n",
        "    test_dataset = create_pyg_dataset(num_nodes=testing_node_size, num_samples=num_test_samples,large_number=large_number)\n",
        "    testing_data_by_sizes[testing_node_size] = test_dataset"
      ],
      "metadata": {
        "id": "CfRcKR7jnAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(0)\n",
        "\n",
        "# Generate training data\n",
        "training_data_by_sizes = {}\n",
        "for training_node_size in training_node_sizes:\n",
        "    num_train_samples = 100\n",
        "    train_dataset = create_pyg_dataset(num_nodes=training_node_size, num_samples=num_train_samples,large_number=large_number)\n",
        "    training_data_by_sizes[training_node_size] = train_dataset"
      ],
      "metadata": {
        "id": "Awfi8GagTbW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training/Testing"
      ],
      "metadata": {
        "id": "f3LAcFQh7vHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training basic setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def runTraining(hidden_channels_choice, seed, alpha, training_node_size):\n",
        "    set_seed(seed)\n",
        "    network = ShortestPathGNN(hidden_channels=hidden_channels_choice, num_layers=K)\n",
        "    network.to(device)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=0.0003)\n",
        "    loss_function = torch.nn.MSELoss()\n",
        "    num_epochs = 1000\n",
        "\n",
        "    train_loader = pyg.loader.DataLoader(training_data_by_sizes[training_node_size], batch_size=64, shuffle=True)\n",
        "\n",
        "    test_loader_temp = pyg.loader.DataLoader(testing_data_by_sizes[14], batch_size=64, shuffle=False) # for testing during training\n",
        "\n",
        "    # Lists to store epoch loss and L1 norm; not saved in these experiments\n",
        "    epoch_losses = []\n",
        "    l1_norms = []\n",
        "\n",
        "    def train_one_epoch(network, optimizer, loss_function, train_loader, alpha, device):\n",
        "        network.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            network.zero_grad()\n",
        "            batch = batch.to(device)\n",
        "            pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "            loss = loss_function(pred.flatten(), batch.y)\n",
        "\n",
        "            # Add L1 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in network.parameters())\n",
        "            loss = loss + alpha * l1_norm\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(training_data_by_sizes[training_node_size])\n",
        "        return epoch_loss, alpha * l1_norm.item(), l1_norm.item()\n",
        "\n",
        "    def test_model(network, loss_function, test_loader, device):\n",
        "        network.eval()\n",
        "        test_loss = 0.\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = batch.to(device)\n",
        "                pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "                loss = loss_function(pred.flatten(), batch.y)\n",
        "                test_loss += loss.item()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        return test_loss\n",
        "\n",
        "    pbar = tqdm(range(num_epochs))\n",
        "    for epoch in pbar:\n",
        "        train_loss, train_l1_norm, l1_norm = train_one_epoch(network, optimizer, loss_function, train_loader, alpha, device)\n",
        "        test_loss_temp = test_model(network, loss_function, test_loader_temp, device)\n",
        "        epoch_losses.append(train_loss)\n",
        "        l1_norms.append(train_l1_norm)\n",
        "        pbar.set_description(f\"Large Number: {large_number}, Alpha: {alpha}, Hidden Channels: {hidden_channels_choice}, Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss_temp:.4f}, L1 Norm: {train_l1_norm:.4f}, Unadjusted L1: {l1_norm:.4f}\")\n",
        "\n",
        "    test_losses = {}\n",
        "\n",
        "    for key,testing_data in testing_data_by_sizes.items():\n",
        "        test_loader = pyg.loader.DataLoader(testing_data, batch_size=64, shuffle=False)\n",
        "        test_loss = test_model(network, loss_function, test_loader, device)\n",
        "        experiment = {\"_type\":0,\"alpha\":alpha,\"Training Data Nodes\":training_node_size,\"Training Data Samples\":len(training_data_by_sizes[training_node_size]),\"Testing Data Nodes\":key,\"Testing Data Samples\":len(testing_data),\"Seed\":seed,\"K\":K,\"Large Number\":large_number}\n",
        "        experiment.update({\"_\":\"|\"})\n",
        "        final_values = {\"Hidden Channels\": hidden_channels_choice,\"Epoch\": epoch, \"Train_Loss\": train_loss, \"Test Loss\": test_loss, \"L1 Norm\": train_l1_norm, \"Unadjusted L1\": l1_norm}\n",
        "        experiment.update(final_values)\n",
        "        add_experiment(experiment)\n",
        "        test_losses[key] = test_loss\n",
        "    return test_losses"
      ],
      "metadata": {
        "id": "831Q_V2NKVrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom experimental setup code:"
      ],
      "metadata": {
        "id": "-dTLQe_6K58w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels = 4\n",
        "alpha = 1.5\n",
        "test_losses_by_training_node_size = {}\n",
        "\n",
        "for training_node_size in training_node_sizes:\n",
        "    test_losses = runTraining(hidden_channels,0,alpha,training_node_size)\n",
        "    test_losses_by_training_node_size[training_node_size] = test_losses"
      ],
      "metadata": {
        "id": "sqPYukCBE7hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses_data = []\n",
        "row_labels = []\n",
        "column_labels = []\n",
        "for key, val in test_losses_by_training_node_size.items():\n",
        "    if not column_labels:\n",
        "        column_labels = list(val.keys())\n",
        "    test_losses_data.append(list(val.values()))\n",
        "    row_labels.append(key)\n",
        "\n",
        "test_losses_array = np.array(test_losses_data)\n",
        "\n",
        "panda_array = pd.DataFrame(test_losses_array, index=row_labels, columns=column_labels)\n",
        "panda_array.to_csv(folder_path + \"test_losses.csv\")"
      ],
      "metadata": {
        "id": "7qC9IQ-lFrCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}