{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "yh3BbGigEKjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install captum # causes dependency issue with numpy as numpy requires a version <2; in colab simply hit restart runtime to use the older version without error\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "BYwVBMBKC_Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viaoM-SzJaTG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from itertools import pairwise, product\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as pyg\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    return seed\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "uGMO6qcPJg5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores data with all values pertaining to their individual training\n",
        "experiments = []\n",
        "\n",
        "def read_csv():\n",
        "    try:\n",
        "        with open(folder_path+file_name, \"r\") as f:\n",
        "            return pd.read_csv(f,index_col=0)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "\n",
        "def add_experiment(param):\n",
        "    experiments.append(param)\n",
        "\n",
        "def get_dataframe():\n",
        "    df = pd.DataFrame(experiments)\n",
        "    return df\n",
        "\n",
        "def clear_experiments():\n",
        "    experiments.clear()\n",
        "\n",
        "def load_experiments(path):\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        for index, row in df.iterrows():\n",
        "            experiments.append(row.to_dict())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No CSV at {path}.\")\n",
        "\n",
        "def save_dataframe(path):\n",
        "    df = get_dataframe()\n",
        "    df.to_csv(path)\n",
        "\n",
        "def insert_divider():\n",
        "    experiments.append({\"_type\":1,\"alpha\":\"-\",\"Training Data Nodes Max\":\"-\",\"Training Data Samples\":\"-\",\"Testing Data Nodes\":\"-\",\"Testing Data Samples\":\"-\",\"Seed\":\"-\",\"K\":\"-\",\"Large Number\":\"-\",\"_\":\"|\",\"Hidden Channels\": \"-\",\"Epoch\": \"-\", \"Train_Loss\": \"-\", \"Test_Loss\": \"-\", \"L1 Norm\": \"-\", \"Unadjusted L1\": \"-\"})"
      ],
      "metadata": {
        "id": "S280qlJFJivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell must change depending on runtime environment; it is currently configured for my Colab.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "folder_path = \"/content/drive/MyDrive/FinDSExperiments/\"\n",
        "file_name = \"GINE_Hyperparamter_Test.csv\" # Should change by experiment"
      ],
      "metadata": {
        "id": "4Q53TE_zJumj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "eiqKCnDrEjx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShortestPathGNN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        num_layers: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in_channels = 1\n",
        "        out_channels = 1\n",
        "        self.encoder = torch.nn.Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            layer = pyg.nn.GINEConv(\n",
        "                nn=torch.nn.Sequential(\n",
        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "                ),edge_dim=1)\n",
        "            self.layers.append(layer)\n",
        "        self.decoder = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor):\n",
        "        x = self.encoder(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, edge_index, edge_attr)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KNFON27fKH6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Data"
      ],
      "metadata": {
        "id": "V7-RVeXw7gxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import KeysView\n",
        "K = 4\n",
        "large_number = 10000\n",
        "\n",
        "def convert_networkx_to_pyg_shortest_path(graph: nx.Graph,large_number:int):\n",
        "    nx.set_edge_attributes(graph, values={e : 1.0 + 0.1*np.random.randn() for e in graph.edges()}, name='edge_attr')\n",
        "    data = pyg.utils.convert.from_networkx(graph)\n",
        "    data.x = torch.Tensor([0] + [large_number for _ in range(data.num_nodes-1)]).unsqueeze(1)\n",
        "    length_dict = nx.shortest_path_length(graph, source=0, weight=\"edge_attr\")\n",
        "    data.y = torch.Tensor([length_dict.get(i, large_number) for i in range(data.num_nodes)])\n",
        "    data.edge_attr = data.edge_attr.unsqueeze(1)\n",
        "    return data\n",
        "\n",
        "def get_connected_ER_graph(num_nodes: int, p: float):\n",
        "    while True: # loop until we generate a connected graph\n",
        "        graph = nx.erdos_renyi_graph(num_nodes, p)\n",
        "        if nx.is_connected(graph):\n",
        "            return graph\n",
        "\n",
        "def create_pyg_dataset(num_nodes: int, num_samples: int, large_number:int):\n",
        "    set_seed(0)\n",
        "    return [\n",
        "        convert_networkx_to_pyg_shortest_path(get_connected_ER_graph(num_nodes, p=0.1),large_number)\n",
        "        for _ in range(num_samples)\n",
        "    ]"
      ],
      "metadata": {
        "id": "vLSiSyZUKLlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training data\n",
        "num_nodes = 20\n",
        "num_samples = 500\n",
        "train_dataset = create_pyg_dataset(num_nodes=num_nodes, num_samples=num_samples, large_number=large_number)\n",
        "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Generate testing data\n",
        "num_test_nodes = 200\n",
        "num_test_samples = 100\n",
        "test_dataset = create_pyg_dataset(num_nodes=num_test_nodes, num_samples=num_test_samples, large_number=large_number)\n",
        "test_loader = pyg.loader.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "CfRcKR7jnAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training/Testing"
      ],
      "metadata": {
        "id": "f3LAcFQh7vHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training basic setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def runTraining(hidden_channels_choice, seed, alpha):\n",
        "    set_seed(seed)\n",
        "    network = ShortestPathGNN(hidden_channels=hidden_channels_choice, num_layers=K)\n",
        "    network.to(device)\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=0.0003)\n",
        "    loss_function = torch.nn.MSELoss()\n",
        "    num_epochs = 250\n",
        "\n",
        "    # Lists to store epoch loss and L1 norm\n",
        "    epoch_losses = []\n",
        "    l1_norms = []\n",
        "    test_losses = []\n",
        "\n",
        "    def train_one_epoch(network, optimizer, loss_function, train_loader, alpha, device):\n",
        "        network.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            network.zero_grad()\n",
        "            batch = batch.to(device)\n",
        "            pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "            loss = loss_function(pred.flatten(), batch.y)\n",
        "\n",
        "            # Add L1 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in network.parameters())\n",
        "            loss = loss + alpha * l1_norm\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(train_loader.dataset)\n",
        "        return epoch_loss, alpha * l1_norm.item(), l1_norm.item()\n",
        "\n",
        "    def test_model(network, loss_function, test_loader, device):\n",
        "        network.eval()\n",
        "        test_loss = 0.\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = batch.to(device)\n",
        "                pred = network(batch.x, batch.edge_index, batch.edge_attr)\n",
        "                loss = loss_function(pred.flatten(), batch.y)\n",
        "                test_loss += loss.item()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        return test_loss\n",
        "\n",
        "    pbar = tqdm(range(num_epochs))\n",
        "    final_values = {}\n",
        "    for epoch in pbar:\n",
        "        train_loss, train_l1_norm, l1_norm = train_one_epoch(network, optimizer, loss_function, train_loader, alpha, device)\n",
        "        test_loss = test_model(network, loss_function, test_loader, device)\n",
        "        epoch_losses.append(train_loss)\n",
        "        l1_norms.append(train_l1_norm)\n",
        "        pbar.set_description(f\"Large Number: {large_number}, Alpha: {alpha}, Hidden Channels: {hidden_channels_choice}, Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, L1 Norm: {train_l1_norm:.4f}, Unadjusted L1: {l1_norm:.4f}\")\n",
        "        final_values = {\"Hidden Channels\": hidden_channels_choice,\"Epoch\": epoch, \"Train_Loss\": train_loss, \"Test_Loss\": test_loss, \"L1 Norm\": train_l1_norm, \"Unadjusted L1\": l1_norm}\n",
        "\n",
        "    experiment = {\"_type\":0,\"alpha\":alpha,\"Training Data Nodes Max\":num_nodes,\"Training Data Samples\":num_samples,\"Testing Data Nodes\":num_test_nodes,\"Testing Data Samples\":num_test_samples,\"Seed\":seed,\"K\":K,\"Large Number\":large_number}\n",
        "    experiment.update({\"_\":\"|\"})\n",
        "    experiment.update(final_values)\n",
        "    add_experiment(experiment)\n",
        "    return final_values"
      ],
      "metadata": {
        "id": "831Q_V2NKVrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Experiments"
      ],
      "metadata": {
        "id": "-dTLQe_6K58w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Search"
      ],
      "metadata": {
        "id": "CyHLn5Uq43y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels_choices = [2,4,8,16,32]\n",
        "seed_choices = range(5)\n",
        "alpha_choices = [0.001,0.005,0.01,0.05,0.1]\n",
        "\n",
        "def last_completed_experiment():\n",
        "    experiment_csv = read_csv()\n",
        "    if experiment_csv is None:\n",
        "        return None\n",
        "    last_experiment_row = experiment_csv[experiment_csv['_type'] == 0].iloc[-1]\n",
        "    return last_experiment_row.drop(\"_\").astype(float)\n",
        "\n",
        "def last_row():\n",
        "    experiment_csv = read_csv()\n",
        "    last_row = experiment_csv.iloc[-1]\n",
        "    return last_row\n",
        "\n",
        "def next_experiment():\n",
        "    last_experiment = last_completed_experiment()\n",
        "    if last_experiment is None:\n",
        "        return[0,0,0]\n",
        "    if last_experiment['Hidden Channels']==hidden_channels_choices[-1]:\n",
        "        if last_experiment['Seed']==seed_choices[-1]:\n",
        "            if last_experiment['alpha']==alpha_choices[-1]:\n",
        "                return None\n",
        "            else:\n",
        "                if last_row()['_type']!=1:\n",
        "                    insert_divider()\n",
        "                    insert_divider()\n",
        "                return[0,0,alpha_choices.index(last_experiment['alpha'])+1]\n",
        "        else:\n",
        "            if last_row()['_type']!=1:\n",
        "                insert_divider()\n",
        "            return[0,seed_choices.index(last_experiment['Seed'])+1,alpha_choices.index(last_experiment['alpha'])]\n",
        "    else:\n",
        "        return[hidden_channels_choices.index(last_experiment['Hidden Channels'])+1,seed_choices.index(last_experiment['Seed']),alpha_choices.index(last_experiment['alpha'])]"
      ],
      "metadata": {
        "id": "TK6dImkB42Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_experiments(folder_path+file_name)\n",
        "\n",
        "next_experiment_to_run = next_experiment()\n",
        "if next_experiment_to_run != None:\n",
        "    for alpha_index in range(next_experiment_to_run[2], len(alpha_choices)):\n",
        "        alpha_choice = alpha_choices[alpha_index]\n",
        "        for seed_index in range(next_experiment_to_run[1] if alpha_index == next_experiment_to_run[2] else 0, len(seed_choices)):\n",
        "            seed_choice = seed_choices[seed_index]\n",
        "            for hidden_channels_index in range(next_experiment_to_run[0] if alpha_index == next_experiment_to_run[2] and seed_index == next_experiment_to_run[1] else 0, len(hidden_channels_choices)):\n",
        "                hidden_channels_choice = hidden_channels_choices[hidden_channels_index]\n",
        "                runTraining(hidden_channels_choice,seed_choice,alpha_choice)\n",
        "            insert_divider()\n",
        "            save_dataframe(folder_path+file_name)\n",
        "        insert_divider()\n",
        "        save_dataframe(folder_path+file_name)\n",
        "else:\n",
        "    print(\"No more experiments to run\")"
      ],
      "metadata": {
        "id": "dluOQQta55AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pseudo-gradient Descent on Alpha"
      ],
      "metadata": {
        "id": "HmD-_fxm6s77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_history = []\n",
        "train_history = []\n",
        "test_history = []\n",
        "current_alpha = 1.5\n",
        "previous_alpha = None"
      ],
      "metadata": {
        "id": "VIE05LvL6rug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels_current_test_value = 2 # Can be changed\n",
        "\n",
        "PSEUDO_DERIVATIVE_JUMP = 1e-5\n",
        "\n",
        "def approximate_derivative(final_values_actual,final_values_jump):\n",
        "    return (final_values_jump['Test_Loss']-final_values_actual['Test_Loss'])/PSEUDO_DERIVATIVE_JUMP\n",
        "\n",
        "num_trials = 20\n",
        "learning_rate = 25\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    experiment_values = runTraining(hidden_channels_current_test_value,0,current_alpha)\n",
        "    current_train_loss = experiment_values['Train_Loss']\n",
        "\n",
        "    derivative_values = runTraining(hidden_channels_current_test_value,0,current_alpha+PSEUDO_DERIVATIVE_JUMP)\n",
        "    derivative_train_loss = derivative_values['Train_Loss']\n",
        "    alpha_history.append(current_alpha)\n",
        "    train_history.append(current_train_loss)\n",
        "    test_history.append(experiment_values['Test_Loss'])\n",
        "\n",
        "    gradient_alpha = approximate_derivative(experiment_values,derivative_values)\n",
        "    current_alpha -= learning_rate * gradient_alpha\n",
        "\n",
        "print(\"Arrived at \", current_alpha)\n",
        "insert_divider()\n",
        "save_dataframe(folder_path+file_name)"
      ],
      "metadata": {
        "id": "sM191iMQ62R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Algorithm"
      ],
      "metadata": {
        "id": "wE-LnQHm992x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real"
      ],
      "metadata": {
        "id": "VdcYubWe98o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_function(alpha_list):\n",
        "    alpha = alpha_list[0]\n",
        "\n",
        "    hidden_channels_choice = 2\n",
        "    seed_choice = 1\n",
        "\n",
        "    runTraining(hidden_channels_choice, seed_choice, alpha)\n",
        "\n",
        "    df = get_dataframe()\n",
        "    last_experiment = df[df['_type'] == 0].iloc[-1]\n",
        "    test_loss = last_experiment['Test Loss']\n",
        "\n",
        "    return test_loss\n",
        "\n",
        "# Search space\n",
        "space = [Real(0.0, 10.0, name='alpha')]\n",
        "\n",
        "# Run the Bayesian optimization\n",
        "result = gp_minimize(\n",
        "    func=objective_function,\n",
        "    dimensions=space,\n",
        "    n_calls=30,\n",
        "    random_state=1,\n",
        "    verbose=True,\n",
        "    x0 = [[1.0]]\n",
        ")\n",
        "\n",
        "print(f\"Best alpha found: {result.x[0]}\")\n",
        "print(f\"Best alpha test loss: {result.fun}\")\n",
        "\n",
        "print(result.x_iters)\n",
        "print(result.func_vals)\n",
        "save_dataframe(folder_path+file_name)"
      ],
      "metadata": {
        "id": "Xpj9TrdH-WKM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}